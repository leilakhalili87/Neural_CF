{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning based Recommender System\n",
    "\n",
    "A recommender system is a system which can predict the future preferences of a set of items for a user and recomment the top items.\n",
    "\n",
    "This notebook uses the  **Neural Collaborative Filtering** method based on the paper \"He, Xiangnan, et al. \"Neural collaborative filtering.\" Proceedings of the 26th international conference on world wide web. 2017\" (https://arxiv.org/abs/1708.05031).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Let's first review traditional approaches. These approached include:  \n",
    "1. **Content based recommendation**: is based on the each item\n",
    "2. **Collaborative filtering** :  is based on user's past behavior\n",
    "    - **user based**: measures the similarity between target users and other users\n",
    "    - **item based**: measure the similarity between the items that target users rates/ interacts with\n",
    "\n",
    "Some terms to learn:\n",
    "\n",
    "- m: number of users\n",
    "\n",
    "- n: number of items\n",
    "\n",
    "- $M_{i,j}$: the utility matrix, denoted user i likes item j\n",
    "\n",
    "- Explicit feedback: how a user rates an item, it is  straigtforward\n",
    "\n",
    "- implicit feedback: provides a heuristic about how a user likes an item, it is abundant, however, unable to find the negative preferances\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Before we start building and training our model, let's do some preprocessing to get the data in the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:53:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:31:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:33:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:32:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:29:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating           timestamp\n",
       "0       1        2     3.5 2005-04-02 23:53:47\n",
       "1       1       29     3.5 2005-04-02 23:31:16\n",
       "2       1       32     3.5 2005-04-02 23:33:39\n",
       "3       1       47     3.5 2005-04-02 23:32:07\n",
       "4       1       50     3.5 2005-04-02 23:29:40"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv('./data/rating.csv', \n",
    "                      parse_dates=['timestamp'])\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2030571 rows of data from 13849 users\n"
     ]
    }
   ],
   "source": [
    "rand_userIds = np.random.choice(ratings['userId'].unique(), \n",
    "                                size=int(len(ratings['userId'].unique())*.1), \n",
    "                                replace=False)\n",
    "\n",
    "ratings = ratings.loc[ratings['userId'].isin(rand_userIds)]\n",
    "\n",
    "print('There are {} rows of data from {} users'.format(len(ratings), len(rand_userIds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define train/test dataset. Using random method won't be a good idea. I will use leave one out method. \n",
    "For each user, the most recent review is used as the test set (i.e. leave one out), while the rest will be used as training data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'] \\\n",
    "                                .rank(method='first', ascending=False)\n",
    "\n",
    "train_ratings = ratings[ratings['rank_latest'] != 1]\n",
    "test_ratings = ratings[ratings['rank_latest'] == 1]\n",
    "\n",
    "# drop columns that we no longer need\n",
    "train_ratings = train_ratings[['userId', 'movieId', 'rating']]\n",
    "test_ratings = test_ratings[['userId', 'movieId', 'rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset does not include the implicit feedback.\n",
    "The ratings in the dataset is the explicit feedback. For getting the implicit feedback I define a binary attribute, here overwrite in \"rating\",  where '1' means there is an interaction between the user and that item, '0' when there is no interaction. For creating the '0' ratings I used negative sampling. Four negative samples for each row of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings.loc[:, 'rating'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensTrainDataset(Dataset):\n",
    "    \"\"\"MovieLens PyTorch Dataset for Training\n",
    "    \n",
    "    Args:\n",
    "        ratings (pd.DataFrame): Dataframe containing the movie ratings\n",
    "        all_movieIds (list): List containing all movieIds\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratings, all_movieIds):\n",
    "        self.users, self.items, self.labels = self.get_dataset(ratings, all_movieIds)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.labels[idx]\n",
    "\n",
    "    def get_dataset(self, ratings, all_movieIds):\n",
    "        users, items, labels = [], [], []\n",
    "        user_item_set = set(zip(ratings['userId'], ratings['movieId']))\n",
    "\n",
    "        num_negatives = 4\n",
    "        for u, i in user_item_set:\n",
    "            users.append(u)\n",
    "            items.append(i)\n",
    "            labels.append(1)\n",
    "            for _ in range(num_negatives):\n",
    "                negative_item = np.random.choice(all_movieIds)\n",
    "                while (u, negative_item) in user_item_set:\n",
    "                    negative_item = np.random.choice(all_movieIds)\n",
    "                users.append(u)\n",
    "                items.append(negative_item)\n",
    "                labels.append(0)\n",
    "\n",
    "        return torch.tensor(users), torch.tensor(items), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering (NCF)\n",
    "In this method the input layer, the user and the item are one-hot encoded. Then they are mapped to the hidden space with embedding layers accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(pl.LightningModule):\n",
    "    \"\"\" Neural Collaborative Filtering (NCF)\n",
    "    \n",
    "        Args:\n",
    "            num_users (int): Number of unique users\n",
    "            num_items (int): Number of unique items\n",
    "            ratings (pd.DataFrame): Dataframe containing the movie ratings for training\n",
    "            all_movieIds (list): List containing all movieIds (train + test)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_users, num_items, ratings, all_movieIds):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_embeddings=num_users, embedding_dim=8)\n",
    "        self.item_embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=8)\n",
    "        self.fc1 = nn.Linear(in_features=16, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.output = nn.Linear(in_features=32, out_features=1)\n",
    "        self.ratings = ratings\n",
    "        self.all_movieIds = all_movieIds\n",
    "        \n",
    "    def forward(self, user_input, item_input):\n",
    "        \n",
    "        # Pass through embedding layers\n",
    "        user_embedded = self.user_embedding(user_input)\n",
    "        item_embedded = self.item_embedding(item_input)\n",
    "\n",
    "        # Concat the two embedding layers\n",
    "        vector = torch.cat([user_embedded, item_embedded], dim=-1)\n",
    "\n",
    "        # Pass through dense layer\n",
    "        vector = nn.ReLU()(self.fc1(vector))\n",
    "        vector = nn.ReLU()(self.fc2(vector))\n",
    "\n",
    "        # Output layer\n",
    "        pred = nn.Sigmoid()(self.output(vector))\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        user_input, item_input, labels = batch\n",
    "        predicted_labels = self(user_input, item_input)\n",
    "        loss = nn.BCELoss()(predicted_labels, labels.view(-1, 1).float())\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(MovieLensTrainDataset(self.ratings, self.all_movieIds),\n",
    "                          batch_size=512, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the NCF model using the class that we have defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = ratings['userId'].max() + 1\n",
    "num_items = ratings['movieId'].max() + 1\n",
    "\n",
    "all_movieIds = ratings['movieId'].unique()\n",
    "\n",
    "model = NCF(num_users, num_items, train_ratings, all_movieIds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leila/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name           | Type      | Params\n",
      "---------------------------------------------\n",
      "0 | user_embedding | Embedding | 1.1 M \n",
      "1 | item_embedding | Embedding | 1.0 M \n",
      "2 | fc1            | Linear    | 1.1 K \n",
      "3 | fc2            | Linear    | 2.1 K \n",
      "4 | output         | Linear    | 33    \n",
      "---------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.642     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2612e102064f2f808bda4424252667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=5, gpus=0, reload_dataloaders_every_epoch=True,\n",
    "                     progress_bar_refresh_rate=50, logger=False, checkpoint_callback=False)\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "\n",
    "For model evaluation Hit Ratio is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ef2f5e0c28449ca85595c7bc0ea631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13849.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Hit Ratio @ 10 is 0.82\n"
     ]
    }
   ],
   "source": [
    "# User-item pairs for testing\n",
    "test_user_item_set = set(zip(test_ratings['userId'], test_ratings['movieId']))\n",
    "\n",
    "# Dict of all items that are interacted with by each user\n",
    "user_interacted_items = ratings.groupby('userId')['movieId'].apply(list).to_dict()\n",
    "\n",
    "hits = []\n",
    "for (u,i) in tqdm(test_user_item_set):\n",
    "    interacted_items = user_interacted_items[u]\n",
    "    not_interacted_items = set(all_movieIds) - set(interacted_items)\n",
    "    selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))\n",
    "    test_items = selected_not_interacted + [i]\n",
    "    \n",
    "    predicted_labels = np.squeeze(model(torch.tensor([u]*100), \n",
    "                                        torch.tensor(test_items)).detach().numpy())\n",
    "    \n",
    "    top10_items = [test_items[i] for i in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    if i in top10_items:\n",
    "        hits.append(1)\n",
    "    else:\n",
    "        hits.append(0)\n",
    "        \n",
    "print(\"The Hit Ratio @ 10 is {:.2f}\".format(np.average(hits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a pretty good Hit Ratio @ 10 score! To put this into context, what this means is that 86% of the users were recommended the actual item (among a list of 10 items) that they eventually interacted with. Not bad!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
